{"nbformat_minor": 1, "cells": [{"source": "# Model Training\n\nModel training step for the project '**Aftershock pattern prediction based on earthquake rupture data for improved seismic hazard assessment**' (pred_seism_aftXYZ). DeVries18 will refer to the article 'Deep learning of aftershock patterns following large earthquakes' by Phoebe M. R. DeVries, Fernanda Vi\u00e9gas, Martin Wattenberg & Brendan J. Meade, and published in Nature in 2018 (https://www.nature.com/articles/s41586-018-0438-y ).\n\nInputs from previous steps of the process model:\n-  'Features_DeVries18.pkl': Features and target of baseline model (DeVries18), as pickle file;\n-  'Features_new.pkl': New features and same target as baseline, as pickle file;\n-  'model_baseline_DeVries18_init.h5': Baseline deep neural net of DeVries18, as (untrained) Keras model HDF5 file;\n-  'model_baseline_DeVries18_simplified_init.h5': Simplified topology for baseline DNN of DeVries18, as (untrained) Keras model HDF5 file;\n-  'model_DNN_init.h5': Deep neural net topology proposed for this project, as (untrained) Keras model HDF5 file;\n-  'model_ANN_init.h5': Shallow artificial neural net proposed for this project, as (untrained) Keras model HDF5 file.\n\nFor comparison with the DeVries study, we will use the same training and test sets ('Training_FileNames.h5', 'Testing_FileNames.h5'), first imported from the Google Drive https://drive.google.com/drive/folders/1c5Rb_6EsuP2XedDjg37bFDyf8AadtGDa, as given in the README.md of https://github.com/phoebemrdevries/Learning-aftershock-location-patterns.\n\n## Split dataset into training and testing sets", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "# @hidden_cell\n# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n# You might want to remove those credentials before you share your notebook.\ncredentials_1 = {\n    'IBM_API_KEY_ID': 'XXXXX',\n    'IAM_SERVICE_ID': 'XXXXX',\n    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'IBM_AUTH_ENDPOINT': 'https://iam.bluemix.net/oidc/token',\n    'BUCKET': 'predseismaftxyz-donotdelete-pr-dfvzajzxij3spi',\n    'FILE': 'Training_FileNames.h5'\n}\n# @hidden_cell\n# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n# You might want to remove those credentials before you share your notebook.\ncredentials_2 = {\n    'IBM_API_KEY_ID': 'XXXXX',\n    'IAM_SERVICE_ID': 'XXXXX',\n    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'IBM_AUTH_ENDPOINT': 'https://iam.bluemix.net/oidc/token',\n    'BUCKET': 'predseismaftxyz-donotdelete-pr-dfvzajzxij3spi',\n    'FILE': 'Testing_FileNames.h5'\n}\n\nfrom ibm_botocore.client import Config\nimport ibm_boto3\n\n#Cloud Object Storage\ncos1 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=credentials_1['IBM_API_KEY_ID'],\n    ibm_service_instance_id=credentials_1['IAM_SERVICE_ID'],\n    ibm_auth_endpoint=credentials_1['IBM_AUTH_ENDPOINT'],\n    config=Config(signature_version='oauth'),\n    endpoint_url=credentials_1['ENDPOINT'])\ncos2 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=credentials_2['IBM_API_KEY_ID'],\n    ibm_service_instance_id=credentials_2['IAM_SERVICE_ID'],\n    ibm_auth_endpoint=credentials_2['IBM_AUTH_ENDPOINT'],\n    config=Config(signature_version='oauth'),\n    endpoint_url=credentials_2['ENDPOINT'])", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 1}, {"source": "#cos1.download_file(Bucket=credentials_1['BUCKET'], Key='Training_FileNames.h5', Filename='Training_FileNames.h5')\n#cos2.download_file(Bucket=credentials_2['BUCKET'], Key='Testing_FileNames.h5', Filename='Testing_FileNames.h5')\n!ls", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Features_DeVries18.pkl\t\t\t     model_DNN_init.h5\r\nFeatures_new.pkl\t\t\t     None0000000.png\r\nLabelledDataset_DeVries18_balanced.pkl\t     pred_seism_aftXYZ\r\nLabelledDataset_DeVries18.pkl\t\t     srcmod2.py\r\nmodel_ANN_init.h5\t\t\t     SRCMOD_cleaned.pkl\r\nmodel_baseline_DeVries18_init.h5\t     Testing_FileNames.h5\r\nmodel_baseline_DeVries18_simplified_init.h5  Training_FileNames.h5\r\n"}], "execution_count": 2}, {"source": "import h5py\nimport numpy as np\nimport pandas as pd\nimport sklearn\n\nh5file1 = h5py.File('Training_FileNames.h5', 'r')\ntraining_filenames = np.array(h5file1.get('file_names_training'))\nh5file2 = h5py.File('Testing_FileNames.h5', 'r')\ntesting_filenames = np.array(h5file2.get('file_names_testing'))\n\ntraining_IDs_temp = map(lambda x: str(x, 'utf-8'), training_filenames)   #from byte to string\ntraining_IDs = list(map(lambda x: 's' + x[0:x.find('_')], training_IDs_temp))  #extract event tag only\ntesting_IDs_temp = map(lambda x: str(x, 'utf-8'), testing_filenames)   #from byte to string\ntesting_IDs = list(map(lambda x: 's' + x[0:x.find('_')], testing_IDs_temp))  #extract event tag only", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 3}, {"source": "Features_DeVries18 = pd.read_pickle('Features_DeVries18.pkl')\nFeatures_new = pd.read_pickle('Features_new.pkl')", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 4}, {"source": "# create training & testing sets\nTrainingSet_DeVries18 = Features_DeVries18.loc[Features_DeVries18['ID'].isin(training_IDs)]\nTestingSet_DeVries18 = Features_DeVries18.loc[Features_DeVries18['ID'].isin(testing_IDs)]\nTrainingSet_new = Features_new.loc[Features_new['ID'].isin(training_IDs)]\nTestingSet_new = Features_new.loc[Features_new['ID'].isin(testing_IDs)]\n\n# ratio of training samples\n[len(TrainingSet_DeVries18), len(TestingSet_DeVries18), len(TrainingSet_DeVries18)/(len(TrainingSet_DeVries18)+len(TestingSet_DeVries18))]", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "[163912, 38732, 0.8088667811531552]"}, "execution_count": 5, "metadata": {}}], "execution_count": 5}, {"source": "# shuffle training sets\nTrainingSet_DeVries18 = sklearn.utils.shuffle(TrainingSet_DeVries18)\nTrainingSet_new = sklearn.utils.shuffle(TrainingSet_new)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 6}, {"source": "## Train baseline model", "cell_type": "markdown", "metadata": {}}, {"source": "TrainingSet_DeVries18.head(10)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>aftershocksyn</th>\n      <th>posabsxx</th>\n      <th>posabsxy</th>\n      <th>posabsyy</th>\n      <th>posabsxz</th>\n      <th>posabsyz</th>\n      <th>posabszz</th>\n      <th>negabsxx</th>\n      <th>negabsxy</th>\n      <th>negabsyy</th>\n      <th>negabsxz</th>\n      <th>negabsyz</th>\n      <th>negabszz</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>222405</th>\n      <td>s2004SUMATR01AMMO</td>\n      <td>0.0</td>\n      <td>0.267183</td>\n      <td>0.112649</td>\n      <td>0.188620</td>\n      <td>0.269481</td>\n      <td>0.204584</td>\n      <td>0.051750</td>\n      <td>-0.267183</td>\n      <td>-0.112649</td>\n      <td>-0.188620</td>\n      <td>-0.269481</td>\n      <td>-0.204584</td>\n      <td>-0.051750</td>\n    </tr>\n    <tr>\n      <th>6693</th>\n      <td>s1998IWATEJ01MIYA</td>\n      <td>0.0</td>\n      <td>0.000023</td>\n      <td>0.000051</td>\n      <td>0.000052</td>\n      <td>0.000017</td>\n      <td>0.000005</td>\n      <td>0.000005</td>\n      <td>-0.000023</td>\n      <td>-0.000051</td>\n      <td>-0.000052</td>\n      <td>-0.000017</td>\n      <td>-0.000005</td>\n      <td>-0.000005</td>\n    </tr>\n    <tr>\n      <th>2480</th>\n      <td>s2009LAQUIL05CIRE</td>\n      <td>0.0</td>\n      <td>0.006977</td>\n      <td>0.019410</td>\n      <td>0.022114</td>\n      <td>0.004669</td>\n      <td>0.006679</td>\n      <td>0.001286</td>\n      <td>-0.006977</td>\n      <td>-0.019410</td>\n      <td>-0.022114</td>\n      <td>-0.004669</td>\n      <td>-0.006679</td>\n      <td>-0.001286</td>\n    </tr>\n    <tr>\n      <th>13750</th>\n      <td>s2003TOKACH01TANI</td>\n      <td>0.0</td>\n      <td>0.047442</td>\n      <td>0.000795</td>\n      <td>0.001304</td>\n      <td>0.013542</td>\n      <td>0.000825</td>\n      <td>0.002027</td>\n      <td>-0.047442</td>\n      <td>-0.000795</td>\n      <td>-0.001304</td>\n      <td>-0.013542</td>\n      <td>-0.000825</td>\n      <td>-0.002027</td>\n    </tr>\n    <tr>\n      <th>23817</th>\n      <td>s2006KURILI01LAYx</td>\n      <td>1.0</td>\n      <td>0.047339</td>\n      <td>0.014700</td>\n      <td>0.016854</td>\n      <td>0.002011</td>\n      <td>0.002729</td>\n      <td>0.005463</td>\n      <td>-0.047339</td>\n      <td>-0.014700</td>\n      <td>-0.016854</td>\n      <td>-0.002011</td>\n      <td>-0.002729</td>\n      <td>-0.005463</td>\n    </tr>\n    <tr>\n      <th>10257</th>\n      <td>s2012SUMATR01HAYE</td>\n      <td>1.0</td>\n      <td>0.470913</td>\n      <td>0.507731</td>\n      <td>0.543225</td>\n      <td>0.217438</td>\n      <td>0.148457</td>\n      <td>0.048391</td>\n      <td>-0.470913</td>\n      <td>-0.507731</td>\n      <td>-0.543225</td>\n      <td>-0.217438</td>\n      <td>-0.148457</td>\n      <td>-0.048391</td>\n    </tr>\n    <tr>\n      <th>152793</th>\n      <td>s2004SUMATR01AMMO</td>\n      <td>0.0</td>\n      <td>0.783027</td>\n      <td>0.885536</td>\n      <td>0.946634</td>\n      <td>0.198831</td>\n      <td>0.303942</td>\n      <td>0.209710</td>\n      <td>-0.783027</td>\n      <td>-0.885536</td>\n      <td>-0.946634</td>\n      <td>-0.198831</td>\n      <td>-0.303942</td>\n      <td>-0.209710</td>\n    </tr>\n    <tr>\n      <th>10010</th>\n      <td>s1998IWATEJ01NAKA</td>\n      <td>0.0</td>\n      <td>0.000225</td>\n      <td>0.000321</td>\n      <td>0.000015</td>\n      <td>0.000956</td>\n      <td>0.000719</td>\n      <td>0.001658</td>\n      <td>-0.000225</td>\n      <td>-0.000321</td>\n      <td>-0.000015</td>\n      <td>-0.000956</td>\n      <td>-0.000719</td>\n      <td>-0.001658</td>\n    </tr>\n    <tr>\n      <th>8081</th>\n      <td>s2007SOLOMO01JIxx</td>\n      <td>0.0</td>\n      <td>0.006057</td>\n      <td>0.006502</td>\n      <td>0.011508</td>\n      <td>0.000932</td>\n      <td>0.000787</td>\n      <td>0.000154</td>\n      <td>-0.006057</td>\n      <td>-0.006502</td>\n      <td>-0.011508</td>\n      <td>-0.000932</td>\n      <td>-0.000787</td>\n      <td>-0.000154</td>\n    </tr>\n    <tr>\n      <th>104967</th>\n      <td>s2010MAULEC01HAYE</td>\n      <td>0.0</td>\n      <td>0.645532</td>\n      <td>0.187960</td>\n      <td>0.335242</td>\n      <td>0.854503</td>\n      <td>0.009194</td>\n      <td>0.092600</td>\n      <td>-0.645532</td>\n      <td>-0.187960</td>\n      <td>-0.335242</td>\n      <td>-0.854503</td>\n      <td>-0.009194</td>\n      <td>-0.092600</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "                       ID  aftershocksyn  posabsxx  posabsxy  posabsyy  \\\n222405  s2004SUMATR01AMMO            0.0  0.267183  0.112649  0.188620   \n6693    s1998IWATEJ01MIYA            0.0  0.000023  0.000051  0.000052   \n2480    s2009LAQUIL05CIRE            0.0  0.006977  0.019410  0.022114   \n13750   s2003TOKACH01TANI            0.0  0.047442  0.000795  0.001304   \n23817   s2006KURILI01LAYx            1.0  0.047339  0.014700  0.016854   \n10257   s2012SUMATR01HAYE            1.0  0.470913  0.507731  0.543225   \n152793  s2004SUMATR01AMMO            0.0  0.783027  0.885536  0.946634   \n10010   s1998IWATEJ01NAKA            0.0  0.000225  0.000321  0.000015   \n8081    s2007SOLOMO01JIxx            0.0  0.006057  0.006502  0.011508   \n104967  s2010MAULEC01HAYE            0.0  0.645532  0.187960  0.335242   \n\n        posabsxz  posabsyz  posabszz  negabsxx  negabsxy  negabsyy  negabsxz  \\\n222405  0.269481  0.204584  0.051750 -0.267183 -0.112649 -0.188620 -0.269481   \n6693    0.000017  0.000005  0.000005 -0.000023 -0.000051 -0.000052 -0.000017   \n2480    0.004669  0.006679  0.001286 -0.006977 -0.019410 -0.022114 -0.004669   \n13750   0.013542  0.000825  0.002027 -0.047442 -0.000795 -0.001304 -0.013542   \n23817   0.002011  0.002729  0.005463 -0.047339 -0.014700 -0.016854 -0.002011   \n10257   0.217438  0.148457  0.048391 -0.470913 -0.507731 -0.543225 -0.217438   \n152793  0.198831  0.303942  0.209710 -0.783027 -0.885536 -0.946634 -0.198831   \n10010   0.000956  0.000719  0.001658 -0.000225 -0.000321 -0.000015 -0.000956   \n8081    0.000932  0.000787  0.000154 -0.006057 -0.006502 -0.011508 -0.000932   \n104967  0.854503  0.009194  0.092600 -0.645532 -0.187960 -0.335242 -0.854503   \n\n        negabsyz  negabszz  \n222405 -0.204584 -0.051750  \n6693   -0.000005 -0.000005  \n2480   -0.006679 -0.001286  \n13750  -0.000825 -0.002027  \n23817  -0.002729 -0.005463  \n10257  -0.148457 -0.048391  \n152793 -0.303942 -0.209710  \n10010  -0.000719 -0.001658  \n8081   -0.000787 -0.000154  \n104967 -0.009194 -0.092600  "}, "execution_count": 7, "metadata": {}}], "execution_count": 7}, {"source": "features = ['posabsxx', 'posabsxy', 'posabsyy', 'posabsxz', 'posabsyz', 'posabszz',\n           'negabsxx', 'negabsxy', 'negabsyy', 'negabsxz', 'negabsyz', 'negabszz']\ntarget = 'aftershocksyn'\nx_train = TrainingSet_DeVries18[features]\ny_train = TrainingSet_DeVries18[target]\nx_test = TestingSet_DeVries18[features]\ny_test = TestingSet_DeVries18[target]\n\n# save for next step of the process model\nx_test.to_pickle(\"TestingSet_X_DeVries18.pkl\")\ny_test.to_pickle(\"TestingSet_y_DeVries18.pkl\")", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 8}, {"source": "import keras\nfrom keras.models import load_model\n\n# same as in DeVries18\nbaselinemodel_DeVries18 = load_model('model_baseline_DeVries18_init.h5')\nbatch_size = 3500\nepochs = 5\n\nhistory = baselinemodel_DeVries18.fit(x_train, y_train,\n                            batch_size = batch_size,\n                            epochs = epochs,\n                            validation_split = 0.1)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "Using TensorFlow backend.\n"}, {"output_type": "stream", "name": "stdout", "text": "Train on 147520 samples, validate on 16392 samples\nEpoch 1/5\n147520/147520 [==============================] - 4s 25us/step - loss: 0.6392 - binary_accuracy: 0.6663 - val_loss: 0.6024 - val_binary_accuracy: 0.7197\nEpoch 2/5\n147520/147520 [==============================] - 4s 24us/step - loss: 0.6069 - binary_accuracy: 0.7028 - val_loss: 0.6024 - val_binary_accuracy: 0.7199\nEpoch 3/5\n147520/147520 [==============================] - 4s 25us/step - loss: 0.5982 - binary_accuracy: 0.7118 - val_loss: 0.5908 - val_binary_accuracy: 0.7201\nEpoch 4/5\n147520/147520 [==============================] - 4s 24us/step - loss: 0.5944 - binary_accuracy: 0.7158 - val_loss: 0.5991 - val_binary_accuracy: 0.7202\nEpoch 5/5\n147520/147520 [==============================] - 4s 26us/step - loss: 0.5913 - binary_accuracy: 0.7181 - val_loss: 0.5941 - val_binary_accuracy: 0.7202\n"}], "execution_count": 9}, {"source": "This is the baseline model CV results - we should aim at achieving similar or greater accuracy ($\\geq$ 72%) for the new model with new set of features. This will however not assure a same or better generalization than the baseline model (AUC = 85%, see next step of the process model).", "cell_type": "markdown", "metadata": {}}, {"source": "baselinemodel_DeVries18.save('model_baseline_DeVries18_trained.h5')", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 10}, {"source": "### Side note\n\nSince the Devries18 model uses a relatively small input layer (12 nodes), a simpler DNN topology should do just fine. We show it, only for illustrative purpose. The model proposed in this project will use another set of features.", "cell_type": "markdown", "metadata": {}}, {"source": "baselinemodel_DeVries18_simplified = load_model('model_baseline_DeVries18_simplified_init.h5')\n\nhistory = baselinemodel_DeVries18_simplified.fit(x_train, y_train,\n                            batch_size = batch_size,\n                            epochs = epochs,\n                            validation_split = 0.1)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Train on 147520 samples, validate on 16392 samples\nEpoch 1/5\n147520/147520 [==============================] - 0s 3us/step - loss: 0.6287 - binary_accuracy: 0.6972 - val_loss: 0.5995 - val_binary_accuracy: 0.7177\nEpoch 2/5\n147520/147520 [==============================] - 0s 2us/step - loss: 0.5984 - binary_accuracy: 0.7109 - val_loss: 0.5844 - val_binary_accuracy: 0.7147\nEpoch 3/5\n147520/147520 [==============================] - 0s 2us/step - loss: 0.5906 - binary_accuracy: 0.7115 - val_loss: 0.5805 - val_binary_accuracy: 0.7157\nEpoch 4/5\n147520/147520 [==============================] - 0s 2us/step - loss: 0.5876 - binary_accuracy: 0.7130 - val_loss: 0.5782 - val_binary_accuracy: 0.7165\nEpoch 5/5\n147520/147520 [==============================] - 0s 2us/step - loss: 0.5859 - binary_accuracy: 0.7142 - val_loss: 0.5767 - val_binary_accuracy: 0.7172\n"}], "execution_count": 11}, {"source": "# note the similar CV accuracy using 12 - 8 - 8 - 1 instead of 12 - 50 - 50 - 50 - 50 - 50 - 50 - 1\nbaselinemodel_DeVries18_simplified.save('model_baseline_DeVries18_simplified_trained.h5')", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 12}, {"source": "## Train new models\n\nTraining new models with the proposed set of features based on geometry and kinematics (minimum distance to mainshock rupture and mean slip on rupture).", "cell_type": "markdown", "metadata": {}}, {"source": "#features = ['mindist', 'dipMean', 'strikeMean', 'slipMean']   #best 4 - 12 - 12 - 1\nfeatures = ['mindist', 'slipMean']   #best 2 - 6 - 6 - 1\ntarget = 'aftershocksyn'\nx_train = TrainingSet_new[features]\ny_train = TrainingSet_new[target]\nx_test = TestingSet_new[features]\ny_test = TestingSet_new[target]\n\n# save for next step of the process model\nx_test.to_pickle(\"TestingSet_X_new.pkl\")\ny_test.to_pickle(\"TestingSet_y_new.pkl\")", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 13}, {"source": "### Train new DNN (simplified topology)", "cell_type": "markdown", "metadata": {}}, {"source": "model_DNN = load_model('model_DNN_init.h5')\n\nhistory = model_DNN.fit(x_train, y_train,\n                            batch_size = batch_size,\n                            epochs = epochs,\n                            validation_split = 0.1)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Train on 147520 samples, validate on 16392 samples\nEpoch 1/5\n147520/147520 [==============================] - 0s 3us/step - loss: 0.6853 - binary_accuracy: 0.6374 - val_loss: 0.6728 - val_binary_accuracy: 0.7247\nEpoch 2/5\n147520/147520 [==============================] - 0s 1us/step - loss: 0.6650 - binary_accuracy: 0.6413 - val_loss: 0.6426 - val_binary_accuracy: 0.7227\nEpoch 3/5\n147520/147520 [==============================] - 0s 1us/step - loss: 0.6359 - binary_accuracy: 0.6477 - val_loss: 0.6053 - val_binary_accuracy: 0.7277\nEpoch 4/5\n147520/147520 [==============================] - 0s 1us/step - loss: 0.6171 - binary_accuracy: 0.6596 - val_loss: 0.5869 - val_binary_accuracy: 0.7343\nEpoch 5/5\n147520/147520 [==============================] - 0s 1us/step - loss: 0.6078 - binary_accuracy: 0.6676 - val_loss: 0.5780 - val_binary_accuracy: 0.7366\n"}], "execution_count": 14}, {"source": "# Documenting a first DNN model result\n# we tried the following topology 4 - 12 - 12 - 1, the 4 features being: ['mindist', 'dipMean', 'strikeMean', 'slipMean']\n# we obtained: \n#Epoch 5/5\n#147801/147801 [==============================] - 0s 2us/step - loss: 0.6134 - binary_accuracy: 0.6829 - \n#                                                           val_loss: 0.5801 - val_binary_accuracy: 0.7228\n\n# additional tuning involved changes in topology, using relu instead of tanh activation, dropout rate change [NOT SHOWN for clarity].", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "model_DNN.save('model_DNN_trained.h5')", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 15}, {"source": "### Train ANN\n\nTraining of an Artificial Neural Network with one hidden layer (no deep learning!).", "cell_type": "markdown", "metadata": {}}, {"source": "model_ANN = load_model('model_ANN_init.h5')\n\nhistory = model_ANN.fit(x_train, y_train,\n                            batch_size = batch_size,\n                            epochs = epochs,\n                            validation_split = 0.1)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Train on 147520 samples, validate on 16392 samples\nEpoch 1/5\n147520/147520 [==============================] - 0s 3us/step - loss: 0.6547 - binary_accuracy: 0.7135 - val_loss: 0.6359 - val_binary_accuracy: 0.7220\nEpoch 2/5\n147520/147520 [==============================] - 0s 1us/step - loss: 0.6212 - binary_accuracy: 0.7232 - val_loss: 0.6095 - val_binary_accuracy: 0.7216\nEpoch 3/5\n147520/147520 [==============================] - 0s 1us/step - loss: 0.5987 - binary_accuracy: 0.7274 - val_loss: 0.5917 - val_binary_accuracy: 0.7296\nEpoch 4/5\n147520/147520 [==============================] - 0s 1us/step - loss: 0.5836 - binary_accuracy: 0.7339 - val_loss: 0.5797 - val_binary_accuracy: 0.7354\nEpoch 5/5\n147520/147520 [==============================] - 0s 1us/step - loss: 0.5735 - binary_accuracy: 0.7367 - val_loss: 0.5718 - val_binary_accuracy: 0.7371\n"}], "execution_count": 16}, {"source": "model_ANN.save('model_ANN_trained.h5')", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 17}, {"source": "### Train XGBoost classifier\n\nTo not only test neural networks, we here test another standard machine learning algorithm. XGBoost has become a method of choice in recent Kaggle competitions (althought this does not mean that it will perform better than neural networks in the present case - \"No Free Lunch\" theorem):", "cell_type": "markdown", "metadata": {}}, {"source": "import xgboost as xgb\nfrom sklearn.grid_search import GridSearchCV\n\nparams = {\n    'max_depth': [5, 7, 10]\n}\ngridsearch = GridSearchCV(estimator = xgb.XGBClassifier(\n                          objective = \"binary:logistic\"\n                          ),\n                        param_grid = params,\n                        scoring='accuracy',\n                        n_jobs=1,\n                        iid=False,\n                        cv=5)\ngridsearch.fit(x_train, y_train)\ngridsearch.grid_scores_, gridsearch.best_params_, gridsearch.best_score_", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\n"}, {"output_type": "execute_result", "data": {"text/plain": "([mean: 0.78262, std: 0.00259, params: {'max_depth': 5},\n  mean: 0.78558, std: 0.00254, params: {'max_depth': 7},\n  mean: 0.78534, std: 0.00197, params: {'max_depth': 10}],\n {'max_depth': 7},\n 0.7855800447128167)"}, "execution_count": 18, "metadata": {}}], "execution_count": 18}, {"source": "#!pip install joblib", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Collecting joblib\n  Downloading https://files.pythonhosted.org/packages/49/d9/4ea194a4c1d0148f9446054b9135f47218c23ccc6f649aeb09fab4c0925c/joblib-0.13.1-py2.py3-none-any.whl (278kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 286kB 3.5MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: joblib\nSuccessfully installed joblib-0.13.1\n"}], "execution_count": 19}, {"source": "from joblib import dump\n\ndump(gridsearch, 'model_XGBoost.joblib') \n!ls", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Features_DeVries18.pkl\t\t\t\tmodel_XGBoost.joblib\r\nFeatures_new.pkl\t\t\t\tNone0000000.png\r\nLabelledDataset_DeVries18_balanced.pkl\t\tpred_seism_aftXYZ\r\nLabelledDataset_DeVries18.pkl\t\t\tsrcmod2.py\r\nmodel_ANN_init.h5\t\t\t\tSRCMOD_cleaned.pkl\r\nmodel_ANN_trained.h5\t\t\t\tTesting_FileNames.h5\r\nmodel_baseline_DeVries18_init.h5\t\tTestingSet_X_DeVries18.pkl\r\nmodel_baseline_DeVries18_simplified_init.h5\tTestingSet_X_new.pkl\r\nmodel_baseline_DeVries18_simplified_trained.h5\tTestingSet_y_DeVries18.pkl\r\nmodel_baseline_DeVries18_trained.h5\t\tTestingSet_y_new.pkl\r\nmodel_DNN_init.h5\t\t\t\tTraining_FileNames.h5\r\nmodel_DNN_trained.h5\r\n"}], "execution_count": 20}, {"source": "", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3.5", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.5", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}